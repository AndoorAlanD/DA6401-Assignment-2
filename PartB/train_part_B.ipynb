{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile train_partB.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nimport wandb\nimport argparse\n\n\nclass ResNetFineTuner(nn.Module):\n    def __init__(self, config, num_classes=10):\n        super(ResNetFineTuner, self).__init__()\n        self.config = config\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        modules = []\n        if config.dropout > 0:\n            modules.append(nn.Dropout(config.dropout))\n        modules.append(nn.Linear(self.model.fc.in_features, config.num_dense))\n\n        if config.batch_norm == \"true\":\n            modules.append(nn.BatchNorm1d(config.num_dense))\n\n        modules.append(self.get_activation(config.activation))\n        modules.append(nn.Linear(config.num_dense, num_classes))\n        self.model.fc = nn.Sequential(*modules)\n\n        for param in self.model.fc.parameters():\n            param.requires_grad = True\n\n        self.model = self.model.to(self.device)\n        self.criterion = nn.CrossEntropyLoss()\n        if config.optimizer == \"adam\":\n            self.optimizer = optim.Adam(self.model.parameters(), lr=config.lr)\n        elif config.optimizer == \"nadam\":\n            self.optimizer = optim.NAdam(self.model.parameters(), lr=config.lr)\n\n        self.train_loader, self.val_loader = self.get_data_loaders()\n\n    def get_activation(self, name):\n        return {\n            \"ReLU\": nn.ReLU(),\n            \"GELU\": nn.GELU(),\n            \"SiLU\": nn.SiLU(),\n            \"Mish\": nn.Mish()\n        }.get(name, nn.ReLU())\n\n    def get_data_loaders(self):\n        mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n        train_transforms = [transforms.Resize((224, 224))]\n        if self.config.data_aug == \"true\":\n            train_transforms += [transforms.RandomHorizontalFlip(), transforms.RandomRotation(15)]\n        train_transforms += [transforms.ToTensor(), transforms.Normalize(mean, std)]\n\n        val_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ])\n\n        train_dataset = datasets.ImageFolder(\"/kaggle/input/dl-assignment-2/inaturalist_12K/train\",\n                                             transform=transforms.Compose(train_transforms))\n        val_dataset = datasets.ImageFolder(\"/kaggle/input/dl-assignment-2/inaturalist_12K/val\",\n                                           transform=val_transforms)\n\n        train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=self.config.batch_size, shuffle=False, num_workers=2)\n\n        return train_loader, val_loader\n\n    def train_model(self, num_epochs=10):\n        for epoch in range(num_epochs):\n            self.model.train()\n            running_loss, correct, total = 0, 0, 0\n            for images, labels in self.train_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                self.optimizer.zero_grad()\n                outputs = self.model(images)\n                loss = self.criterion(outputs, labels)\n                loss.backward()\n                self.optimizer.step()\n\n                running_loss += loss.item() * images.size(0)\n                _, preds = torch.max(outputs, 1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n\n            train_loss = running_loss / total\n            train_acc = correct / total\n\n            val_loss, val_acc = self.validate()\n            wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc * 100, \"val_loss\": val_loss, \"val_acc\": val_acc * 100})\n\n            print(f\"Epoch {epoch + 1}: Train Acc: {train_acc * 100:.2f}%, Val Acc: {val_acc * 100:.2f}%, Val Loss: {val_loss:.4f}\")\n\n        torch.save(self.model.state_dict(), \"finetuned_resnet50.pth\")\n\n    def validate(self):\n        self.model.eval()\n        val_loss = 0\n        val_correct, val_total = 0, 0\n        with torch.no_grad():\n            for images, labels in self.val_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                outputs = self.model(images)\n                loss = self.criterion(outputs, labels)\n                val_loss += loss.item() * images.size(0)\n                _, preds = torch.max(outputs, 1)\n                val_correct += (preds == labels).sum().item()\n                val_total += labels.size(0)\n\n        return val_loss / val_total, val_correct / val_total\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Fine-tune ResNet50 with configurable options and W&B logging\")\n    parser.add_argument(\"-wp\", \"--wandb_project\", type=str, required=True)\n    parser.add_argument(\"-we\", \"--wandb_entity\", type=str, required=True)\n    parser.add_argument(\"--dropout\", type=float, default=0.3)\n    parser.add_argument(\"--lr\", type=float, default=0.001)\n    parser.add_argument(\"--activation\", type=str, default=\"Mish\", choices=[\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"])\n    parser.add_argument(\"--optimizer\", type=str, default=\"adam\", choices=[\"adam\", \"nadam\"])\n    parser.add_argument(\"--batch_norm\", type=str, default=\"true\", choices=[\"true\", \"false\"])\n    parser.add_argument(\"--data_aug\", type=str, default=\"true\", choices=[\"true\", \"false\"])\n    parser.add_argument(\"--batch_size\", type=int, default=128)\n    parser.add_argument(\"--num_dense\", type=int, default=128)\n    parser.add_argument(\"--epochs\", type=int, default=10)\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    wandb.init(project=args.wandb_project, entity=args.wandb_entity, config=vars(args))\n    wandb.run.name = (\n        f\"{args.activation}-{args.optimizer}-bn_{args.batch_norm}-da_{args.data_aug}-do_{args.dropout}\"\n        f\"-bs_{args.batch_size}-lr_{args.lr}-fc_{args.num_dense}\"\n    )\n\n    model = ResNetFineTuner(args)\n    model.train_model(num_epochs=args.epochs)\n    wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"python train_resnet.py --wandb_project DL_A2 --wandb_entity alandandoor-iit-madras --dropout 0.3 --lr 0.001 --activation Mish --optimizer adam --batch_norm true --data_aug true --batch_size 128 --num_dense 128\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}