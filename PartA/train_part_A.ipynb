{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11458118,"sourceType":"datasetVersion","datasetId":7179445}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile train_partA.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport argparse\nimport wandb\nimport os\n\nos.environ['WANDB_API_KEY'] = 'put your api key here before run'\nwandb.login()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass CNN(nn.Module):\n    def __init__(self, config, num_classes=10):\n        super(CNN, self).__init__()\n        self.config = config\n        self.num_epochs = config.num_epochs\n\n        \n\n        self.build_transforms()\n        self.prepare_data()\n        self.build_model(num_classes)\n        self.to(device)\n        self.build_training_utils()\n\n    def build_transforms(self):\n        base_transform = [\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.5,), std=(0.5,))\n        ]\n\n        augmented_transform = [\n            transforms.Resize((256, 256)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.RandomRotation(20),\n            transforms.ColorJitter(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.5,), std=(0.5,))\n        ]\n\n        self.transform = transforms.Compose(base_transform)\n        self.transform_aug = transforms.Compose(augmented_transform)\n\n    def prepare_data(self):\n        train_transform = self.transform_aug if self.config.data_aug else self.transform\n\n        self.train_dataset = torchvision.datasets.ImageFolder(\n            root=self.config.data_path + '/train', transform=train_transform)\n        self.train_dataset, self.val_dataset = torch.utils.data.random_split(\n            self.train_dataset, [7999, 2000])\n\n        self.train_loader = torch.utils.data.DataLoader(\n            self.train_dataset, batch_size=self.config.batch_size, shuffle=True)\n        self.val_loader = torch.utils.data.DataLoader(\n            self.val_dataset, batch_size=self.config.batch_size, shuffle=True)\n\n    def build_model(self, num_classes):\n        filt_scale = {'half': 0.5, 'double': 2, 'same': 1}[self.config.filt_org]\n\n        inp_fl = 3\n        out_fl = self.config.num_filters\n        self.convL1 = nn.Conv2d(inp_fl, out_fl, self.config.kernel_size[0], stride=1, padding=1)\n        self.batN1 = nn.BatchNorm2d(out_fl)\n\n        inp_fl = out_fl\n        out_fl = int(out_fl * filt_scale)\n        self.convL2 = nn.Conv2d(inp_fl, out_fl, self.config.kernel_size[1], stride=1, padding=1)\n        self.batN2 = nn.BatchNorm2d(out_fl)\n\n        inp_fl = out_fl\n        out_fl = int(out_fl * filt_scale)\n        self.convL3 = nn.Conv2d(inp_fl, out_fl, self.config.kernel_size[2], stride=1, padding=1)\n        self.batN3 = nn.BatchNorm2d(out_fl)\n\n        inp_fl = out_fl\n        out_fl = int(out_fl * filt_scale)\n        self.convL4 = nn.Conv2d(inp_fl, out_fl, self.config.kernel_size[3], stride=1, padding=1)\n        self.batN4 = nn.BatchNorm2d(out_fl)\n\n        inp_fl = out_fl\n        out_fl = int(out_fl * filt_scale)\n        self.convL5 = nn.Conv2d(inp_fl, out_fl, self.config.kernel_size[4], stride=1, padding=1)\n        self.batN5 = nn.BatchNorm2d(out_fl)\n\n        self.maxPool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n\n        img_size = 256\n        for k in self.config.kernel_size:\n            img_size = (img_size - k + 3) // 2\n\n        self.x_shape = out_fl * img_size * img_size\n\n        self.f_Conn = nn.Linear(self.x_shape, self.config.num_dense)\n        self.batN_de = nn.BatchNorm1d(self.config.num_dense)\n        self.dropout = nn.Dropout(p=self.config.dropout)\n        self.opL = nn.Linear(self.config.num_dense, num_classes)\n\n        self.activation = getattr(F, self.config.activation.lower())\n\n    def build_training_utils(self):\n        self.criterion = nn.CrossEntropyLoss()\n        optimizers = {'adam': optim.Adam, 'nadam': optim.NAdam}\n        self.optimizer = optimizers[self.config.optimizer](self.parameters(), lr=self.config.lr)\n\n    def forward(self, x):\n        y = self.config.batch_norm\n        x = self.activation(self.convL1(x))\n        if y: x = self.batN1(x)\n        x = self.maxPool(x)\n\n        x = self.activation(self.convL2(x))\n        if y: x = self.batN2(x)\n        x = self.maxPool(x)\n\n        x = self.activation(self.convL3(x))\n        if y: x = self.batN3(x)\n        x = self.maxPool(x)\n\n        x = self.activation(self.convL4(x))\n        if y: x = self.batN4(x)\n        x = self.maxPool(x)\n\n        x = self.activation(self.convL5(x))\n        if y: x = self.batN5(x)\n        x = self.maxPool(x)\n\n        x = x.view(-1, self.x_shape)\n        x = self.activation(self.f_Conn(x))\n        if y: x = self.batN_de(x)\n        x = self.dropout(x)\n        return self.opL(x)\n\n    def accuracy(self, loader):\n        correct, total, loss = 0, 0, 0\n        self.eval()\n        with torch.no_grad():\n            for x, y in loader:\n                x, y = x.to(device), y.to(device)\n                outputs = self(x)\n                _, preds = torch.max(outputs.data, 1)\n                correct += (preds == y).sum().item()\n                total += y.size(0)\n                loss += self.criterion(outputs, y).item() * y.size(0)\n        self.train()\n        return correct / total, loss / total\n\n    def train_model(self):\n        for epoch in range(self.num_epochs):\n            total_loss = 0\n            correct = 0\n            total = 0\n            for i, (x, y) in enumerate(self.train_loader):\n                x, y = x.to(device), y.to(device)\n                outputs = self(x)\n                loss = self.criterion(outputs, y)\n\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n                total_loss += loss.item()\n                _, preds = torch.max(outputs.data, 1)\n                correct += (preds == y).sum().item()\n                total += y.size(0)\n                if (i+1)%25 == 0:\n                    print(f\"Epoch [{epoch + 1}/{self.num_epochs}]|| Step {i + 1}\")\n\n            train_acc = 100 * correct / total\n            train_loss = total_loss / len(self.train_loader)\n\n            val_acc, val_loss = self.accuracy(self.val_loader)\n            val_acc *= 100\n\n            print(f\"Epoch {epoch+1}/{self.num_epochs}\")\n            print(f\"Train Accuracy: {train_acc:.2f}%\\tTrain Loss: {train_loss:.4f}\")\n            print(f\"Validation Accuracy: {val_acc:.2f}%\\tValidation Loss: {val_loss:.4f}\\n\")\n            wandb.log({\"train_accuracy\": train_acc,\"train_loss\": train_loss,\"val_accuracy\": val_acc,\"val_loss\": val_loss})\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-wp\", \"--wandb_project\", type=str, required=True)\n    parser.add_argument(\"-we\", \"--wandb_entity\", type=str, required=True)\n    parser.add_argument('--data_path', type=str, required=True)\n    parser.add_argument('--kernel_size', type=int, nargs=5, default=[3, 3, 3, 3, 3])\n    parser.add_argument('--num_epochs', type=int, default=10)\n    parser.add_argument('--dropout', type=float, default=0.3)\n    parser.add_argument('--lr', type=float, default=0.001)\n    parser.add_argument('--activation', type=str, choices=['ReLU', 'GELU', 'SiLU', 'Mish'], default='Mish')\n    parser.add_argument('--optimizer', type=str, choices=['adam', 'nadam'], default='adam')\n    parser.add_argument('--batch_norm', action='store_true')\n    parser.add_argument('--data_aug', action='store_true')\n    parser.add_argument('--filt_org', choices=['same', 'double', 'half'], default='same')\n    parser.add_argument('--num_filters', type=int, default=64)\n    parser.add_argument('--batch_size', type=int, default=128)\n    parser.add_argument('--num_dense', type=int, default=128)\n    return parser.parse_args()\n\ndef main():\n    config = parse_args()\n\n    wandb.init(project=config.wandb_project, entity=config.wandb_entity, config=vars(config))\n\n    bn = int(config.batch_norm)\n    da = int(config.data_aug)\n    ks = ''.join(str(config.kernel_size[i]) for i in range(5))\n    wandb.run.name = (\n        f\"{config.activation}-{config.optimizer}-bn_{bn}-da_{da}-do_{config.dropout}-bs_{config.batch_size}\"\n        f\"-lr_{config.lr}-f_{config.num_filters}-{config.filt_org}-ks_{ks}-fc_{config.num_dense}\"\n    )\n\n    model = CNN(config, num_classes=10)\n    model.train_model()\n    wandb.finish()\n\n\n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:18:32.226214Z","iopub.execute_input":"2025-04-19T16:18:32.226877Z","iopub.status.idle":"2025-04-19T16:18:32.235824Z","shell.execute_reply.started":"2025-04-19T16:18:32.226852Z","shell.execute_reply":"2025-04-19T16:18:32.235010Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_partA.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!python train_partA.py --wandb_entity alandandoor-iit-madras --wandb_project DL_A2 --data_path /kaggle/input/dl-assignment-2/inaturalist_12K","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:18:36.102795Z","iopub.execute_input":"2025-04-19T16:18:36.103579Z","iopub.status.idle":"2025-04-19T16:43:49.573888Z","shell.execute_reply.started":"2025-04-19T16:18:36.103548Z","shell.execute_reply":"2025-04-19T16:43:49.573192Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malandandoor\u001b[0m (\u001b[33malandandoor-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250419_161841-vfvbky7n\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdevoted-forest-587\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/alandandoor-iit-madras/DL_A2\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/alandandoor-iit-madras/DL_A2/runs/vfvbky7n\u001b[0m\nEpoch [1/10]|| Step 25\nEpoch [1/10]|| Step 50\nEpoch 1/10\nTrain Accuracy: 18.54%\tTrain Loss: 2.2090\nValidation Accuracy: 22.95%\tValidation Loss: 2.0955\n\nEpoch [2/10]|| Step 25\nEpoch [2/10]|| Step 50\nEpoch 2/10\nTrain Accuracy: 27.28%\tTrain Loss: 2.0356\nValidation Accuracy: 29.70%\tValidation Loss: 1.9983\n\nEpoch [3/10]|| Step 25\nEpoch [3/10]|| Step 50\nEpoch 3/10\nTrain Accuracy: 31.75%\tTrain Loss: 1.9429\nValidation Accuracy: 31.75%\tValidation Loss: 1.9358\n\nEpoch [4/10]|| Step 25\nEpoch [4/10]|| Step 50\nEpoch 4/10\nTrain Accuracy: 33.47%\tTrain Loss: 1.8878\nValidation Accuracy: 32.70%\tValidation Loss: 1.8944\n\nEpoch [5/10]|| Step 25\nEpoch [5/10]|| Step 50\nEpoch 5/10\nTrain Accuracy: 35.95%\tTrain Loss: 1.8163\nValidation Accuracy: 33.45%\tValidation Loss: 1.8717\n\nEpoch [6/10]|| Step 25\nEpoch [6/10]|| Step 50\nEpoch 6/10\nTrain Accuracy: 39.23%\tTrain Loss: 1.7387\nValidation Accuracy: 36.05%\tValidation Loss: 1.8441\n\nEpoch [7/10]|| Step 25\nEpoch [7/10]|| Step 50\nEpoch 7/10\nTrain Accuracy: 42.08%\tTrain Loss: 1.6486\nValidation Accuracy: 35.00%\tValidation Loss: 1.8533\n\nEpoch [8/10]|| Step 25\nEpoch [8/10]|| Step 50\nEpoch 8/10\nTrain Accuracy: 46.01%\tTrain Loss: 1.5213\nValidation Accuracy: 36.45%\tValidation Loss: 1.8492\n\nEpoch [9/10]|| Step 25\nEpoch [9/10]|| Step 50\nEpoch 9/10\nTrain Accuracy: 52.57%\tTrain Loss: 1.3630\nValidation Accuracy: 36.70%\tValidation Loss: 1.9035\n\nEpoch [10/10]|| Step 25\nEpoch [10/10]|| Step 50\nEpoch 10/10\nTrain Accuracy: 58.99%\tTrain Loss: 1.1654\nValidation Accuracy: 36.20%\tValidation Loss: 2.0201\n\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▃▄▄▅▅▆▇█\n\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▆▅▅▄▃▂▁\n\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▄▅▆▆█▇███\n\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▅▄▂▂▁▁▁▃▆\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 58.99487\n\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.16545\n\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 36.2\n\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.02009\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mMish-adam-bn_0-da_0-do_0.3-bs_128-lr_0.001-f_64-same-ks_33333-fc_128\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/alandandoor-iit-madras/DL_A2/runs/vfvbky7n\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/alandandoor-iit-madras/DL_A2\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250419_161841-vfvbky7n/logs\u001b[0m\n","output_type":"stream"}],"execution_count":7}]}